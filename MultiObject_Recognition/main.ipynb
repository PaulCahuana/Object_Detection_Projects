{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e53f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, pickle, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.util import *\n",
    "from tracking.tracker import Tracker\n",
    "from datetime import datetime\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629f530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentUrl E:\\ANACONDA\\PRUEBAS\\MultiObject Recognition\\box2vec\n",
      "parentUrl E:\\ANACONDA\\PRUEBAS\\MultiObject Recognition\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from box2vec.resnet import Resnet\n",
    "from box2vec.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ea51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentUrl = \"E:\\ANACONDA\\PRUEBAS\\MultiObject Recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c28745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caps_and_pickles(video_files, detection_files):\n",
    "    caps = []\n",
    "    detections = []\n",
    "    for video_file, detection_file in zip(video_files, detection_files):\n",
    "        caps.append(cv2.VideoCapture(video_file))\n",
    "        detections.append(np.load(detection_file))\n",
    "    return caps, detections, len(detections[0])\n",
    "\n",
    "def get_frames_and_boxes(caps, detections, index, show=True):\n",
    "    frames = []\n",
    "    local_bboxes = []\n",
    "    for camera_id in range(len(caps)):\n",
    "        cap = caps[camera_id]\n",
    "        cap.set(1, index)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            raise\n",
    "\n",
    "        frames.append(frame)\n",
    "        \n",
    "        local_bbox = detections[camera_id][index][:,:-1] # remove p\n",
    "        local_bboxes.append(local_bbox)\n",
    "        if show:\n",
    "            for box in local_bbox:\n",
    "                cv2.rectangle(\n",
    "                            frame, \n",
    "                            (box[0], box[1]), \n",
    "                            (box[2], box[3]), \n",
    "                            (255,0,0),\n",
    "                            1)\n",
    "            cv2.imshow('camera'+str(camera_id), frame)\n",
    "            cv2.waitKey(1)\n",
    "    return frames, local_bboxes\n",
    "\n",
    "def get_embedding(local_bboxes, frames, box_to_vect, sess):\n",
    "    padded_local_bboxes = []\n",
    "    for camera_id, local_bbox in enumerate(local_bboxes):\n",
    "        camera_id_pad = np.empty(shape=(local_bbox.shape[0], 1))\n",
    "        camera_id_pad.fill(camera_id)\n",
    "        local_bbox = np.concatenate((local_bbox, camera_id_pad), axis=1)\n",
    "        padded_local_bboxes.append(local_bbox)\n",
    "    if len(padded_local_bboxes) == 0:\n",
    "        return\n",
    "    bbox_batch = np.concatenate(padded_local_bboxes, axis=0)\n",
    "    if bbox_batch.shape[0] == 0:\n",
    "        return\n",
    "    bbox_batch[:, 0] = bbox_batch[:, 0] / frames[0].shape[1] \n",
    "    bbox_batch[:, 2] = bbox_batch[:, 2] / frames[0].shape[1] \n",
    "    bbox_batch[:, 1] = bbox_batch[:, 1] / frames[0].shape[0] \n",
    "    bbox_batch[:, 3] = bbox_batch[:, 3] / frames[0].shape[0] \n",
    "\n",
    "    image_batch = []\n",
    "    for frame in frames:\n",
    "        image = cv2.resize(frame, (Config['image_width'] , Config['image_height']))\n",
    "        # image = image[..., ::-1] # RGB -> BGR\n",
    "        image_batch.append(image)\n",
    "    image_batch = np.array(image_batch, dtype=np.uint8)\n",
    "    box_ind_batch = bbox_batch[:, -1].astype(np.int32)\n",
    "    # print('image_batch', image_batch.shape)\n",
    "    # print('bbox_batch', bbox_batch.shape)\n",
    "    # print('box_ind_batch', box_ind_batch.shape)\n",
    "    embedding = box_to_vect.inference(image_batch, bbox_batch, box_ind_batch, sess)\n",
    "    # print('embedding')\n",
    "    # print(embedding)\n",
    "    embedding = np.reshape(embedding, [-1, 128])\n",
    "    return embedding\n",
    "\n",
    "def init_box_to_vect_net(model_file):\n",
    "    box_to_vect = Resnet(\n",
    "                    image_height=Config['image_height'], \n",
    "                    image_width=Config['image_width'], \n",
    "                    vector_dim=128, \n",
    "                    alpha=Config['alpha'], \n",
    "                    feature_map_layer = 'block_layer3',\n",
    "                    resnet_size=18,\n",
    "                    data_format='channels_first', \n",
    "                    mode='test', \n",
    "                    # init_learning_rate=0.001,\n",
    "                    # optimizer_name='adam',\n",
    "                    # batch_size=Config['batch_size'],\n",
    "                    # max_step=100000,\n",
    "                    # model_path=r'F:\\ubuntu\\multi-camera-detection_v2\\model',\n",
    "                    # logdir=r'F:\\ubuntu\\multi-camera-detection_v2\\log',\n",
    "                    )\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_file)\n",
    "    print('Load weights from', model_file, 'for box_to_vect!')\n",
    "    return box_to_vect, sess\n",
    "\n",
    "def l2_distance(embedding1, embedding2):\n",
    "\n",
    "    extend_embedding1 = np.repeat(embedding1, embedding2.shape[0], axis = 0)\n",
    "    extend_embedding2 = np.tile(embedding2, (embedding1.shape[0],1))\n",
    "    distance = np.sqrt(np.sum(np.square(extend_embedding1 - extend_embedding2), axis=1))\n",
    "    distance = distance.reshape([embedding1.shape[0], embedding2.shape[0]])\n",
    "    return distance\n",
    "\n",
    "def fusion(distance, distance_threshold):\n",
    "    pairs = np.argwhere(distance < distance_threshold)\n",
    "    cluster = Disjoint()\n",
    "    for i in range(distance.shape[0]):\n",
    "        cluster.create_set(i)\n",
    "\n",
    "    for pair in pairs:\n",
    "        cluster.merge_sets(pair[0], pair[1])\n",
    "    return cluster\n",
    "\n",
    "def fusion_cluster(bboxes, method='cluster', distance_threshold=0.1, nms_threshold=0.3):\n",
    "    '''\n",
    "        bboxes: bboxes to be fused\n",
    "        method: 'cluster' or 'nms'\n",
    "    '''\n",
    "    if bboxes.shape[0]<=1:\n",
    "        return Disjoint(), bboxes\n",
    "    if method == 'cluster':\n",
    "        clusters = hcluster.fclusterdata(bboxes, distance_threshold, criterion=\"distance\", depth=2)\n",
    "        print('clusters', clusters)\n",
    "        cluster_num = len(set(clusters))\n",
    "        cluster_set_data = [[] for _ in range(cluster_num)]\n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            cluster_set_data[cluster_id-1].append(i)\n",
    "\n",
    "        cluster_count = np.zeros((cluster_num, 1), dtype=np.int32)\n",
    "        cluster_set = Disjoint()\n",
    "        cluster_set.sets = cluster_set_data\n",
    "        fused_bboxes = np.zeros((cluster_num, bboxes.shape[1]))\n",
    "        for k in range(bboxes.shape[0]):\n",
    "           fused_bboxes[clusters[k]-1] += bboxes[k]\n",
    "           cluster_count[clusters[k]-1] += 1\n",
    "        fused_bboxes /= cluster_count\n",
    "        # print(cluster_count)\n",
    "        # print('fused_bboxes', fused_bboxes.shape)\n",
    "        # print(np.where(cluster_count>1))\n",
    "        # fused_bboxes = fused_bboxes[np.where(cluster_count>1)[0]]\n",
    "        return cluster_set, fused_bboxes # (n, 4)\n",
    "    elif method == 'nms':\n",
    "        return nms(bboxes, nms_threshold)\n",
    "    else:\n",
    "        logging.error(method+'Not Implement yet')\n",
    "        raise\n",
    "\n",
    "def demo(save=True, use_cluster=True):\n",
    "    file_names = [\n",
    "        '4p-c0', '4p-c1', '4p-c2', '4p-c3'\n",
    "    ]\n",
    "\n",
    "    video_files = [\n",
    "        os.path.join(currentUrl, 'data', 'train', 'lab', file_name+'.avi')\\\n",
    "            for file_name in file_names \n",
    "    ]\n",
    "\n",
    "    detection_files = [\n",
    "        os.path.join(currentUrl, 'data', 'train', 'lab', file_name+'.pickle')\\\n",
    "            for file_name in file_names\n",
    "    ]\n",
    "\n",
    "    model_file = os.path.join(currentUrl, 'box2vec', 'model', 'model-86000')\n",
    "    cluster_distance_threshold = 0.7\n",
    "    \n",
    "    print('video_files:')\n",
    "    print(video_files)\n",
    "    print('detection_files:')\n",
    "    print(detection_files)\n",
    "    caps, detections, video_length = get_caps_and_pickles(video_files, detection_files)\n",
    "    if save:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        now_str = str(datetime.now()).replace(':', '-')[:-7]\n",
    "        cwd = os.getcwd()\n",
    "\n",
    "        save_dir = os.path.join(cwd, 'result', now_str)\n",
    "        os.mkdir(save_dir)\n",
    "        video_names = [os.path.join(save_dir,'c_'+str(i)+'.avi') for i in range(len(caps))]\n",
    "        outers = [cv2.VideoWriter(video_names[i], fourcc, 30.0, (360, 288)) for i in range(len(caps))]\n",
    "    box_to_vect, sess = init_box_to_vect_net(model_file)\n",
    "    tracker = Tracker(\n",
    "                        traj_smoth_alpha = 0.99, \n",
    "                        image_boundary=None,\n",
    "                        lost_times_thresh=30, \n",
    "                        lost_times_thresh_edge=3,\n",
    "                        appear_times_thresh=30,\n",
    "                        assoc_score_thresh = 0.7,\n",
    "                        cost_metric='distance'\n",
    "                        )\n",
    "\n",
    "    for frame_id in range(100, video_length):\n",
    "        print('-'*60)\n",
    "        print('frame_id:', frame_id)\n",
    "        frames, boxes = get_frames_and_boxes(caps, detections, frame_id, show=False)\n",
    "        embedding = get_embedding(boxes, frames, box_to_vect, sess)\n",
    "        if embedding is None:\n",
    "            continue\n",
    "        if embedding.shape[0] > 1:\n",
    "            embedding_tsne = TSNE().fit_transform(embedding)\n",
    "            plt.scatter(embedding_tsne[:, 0], embedding_tsne[:, 1])\n",
    "            plt.pause(1)\n",
    "            plt.close('all')\n",
    "        if use_cluster:\n",
    "            cluster_set, fused_embeddings = fusion_cluster(bboxes=embedding, method='cluster', distance_threshold=1, nms_threshold=0.3)\n",
    "\n",
    "        else:\n",
    "            distance = l2_distance(embedding, embedding)\n",
    "            print('distance:')\n",
    "            print(distance)\n",
    "            cluster_set = fusion(distance, distance_threshold=cluster_distance_threshold)\n",
    "            print('cluster_sets:', len(cluster_set.sets))\n",
    "            fused_embeddings = []\n",
    "            for embedding_ids in cluster_set.sets:\n",
    "                clustered_embedding = embedding[embedding_ids, :]\n",
    "                fused_embeddings.append(np.mean(clustered_embedding, axis=0))\n",
    "            fused_embeddings = np.concatenate(fused_embeddings, axis=0)\n",
    "            fused_embeddings = np.reshape(fused_embeddings, [-1, 128])\n",
    "            print('fused_embeddings:')\n",
    "            print(fused_embeddings.shape)\n",
    "\n",
    "        tracker.update_tracker(candidate_bboxes_original=fused_embeddings, time_stamp=frame_id)\n",
    "        obj_to_traj = {}\n",
    "        for valid_index in tracker.real_alive_index:\n",
    "            print('valid_index:',valid_index)\n",
    "            traj = tracker.trajectories[valid_index]\n",
    "            current_object_id = traj.object_id[-1]\n",
    "            traj_serial_num = tracker.whole_real_alive_index.index(valid_index)\n",
    "            obj_to_traj[current_object_id] = traj_serial_num\n",
    "            #print(traj_serial_num, ':', current_object_id)\n",
    "\n",
    "        print(obj_to_traj)\n",
    "\n",
    "        global_object_id_to_traj_id = [-1] * embedding.shape[0]\n",
    "        for obj_id, cluster in enumerate(cluster_set.sets):\n",
    "            for global_object_id in cluster:\n",
    "                if obj_id in obj_to_traj:\n",
    "                    global_object_id_to_traj_id[global_object_id] = obj_to_traj[obj_id]\n",
    "        global_object_id = 0\n",
    "        for camera_id in range(4):\n",
    "            frame = frames[camera_id]\n",
    "            local_bbox = boxes[camera_id]\n",
    "            for box in local_bbox:\n",
    "                traj_id = global_object_id_to_traj_id[global_object_id]\n",
    "                # print(traj_id)\n",
    "                np.random.seed(traj_id+10)\n",
    "                color = np.random.randint(256, size=3)\n",
    "                cv2.rectangle(\n",
    "                            frame, \n",
    "                            (box[0], box[1]), \n",
    "                            (box[2], box[3]), \n",
    "                            (int(color[0]), int(color[1]), int(color[2])),\n",
    "                            1)\n",
    "                \n",
    "                global_object_id += 1\n",
    "                cv2.putText(frame, str(traj_id), (box[0], box[3]),\\\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 0.8, (int(color[0]), int(color[1]), int(color[2])), thickness = 2, lineType = -1)\n",
    "            cv2.putText(frame, str(frame_id), (0, 20),\\\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 0, 255), thickness=2, lineType=-1)\n",
    "            # print(frame.shape)\n",
    "            if save:\n",
    "                outers[camera_id].write(frame)\n",
    "            cv2.imshow('camera'+str(camera_id), frame)\n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9e414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_files:\n",
      "['E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c0.avi', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c1.avi', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c2.avi', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c3.avi']\n",
      "detection_files:\n",
      "['E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c0.pickle', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c1.pickle', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c2.pickle', 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c3.pickle']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c0.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-49e5d1b762ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f4f391381163>\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(save, use_cluster)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'detection_files:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetection_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mcaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_caps_and_pickles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mfourcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;34m'XVID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f4f391381163>\u001b[0m in \u001b[0;36mget_caps_and_pickles\u001b[1;34m(video_files, detection_files)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetection_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcaps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdetections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetection_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\GPU\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\ANACONDA\\\\PRUEBAS\\\\MultiObject Recognition\\\\data\\\\train\\\\lab\\\\4p-c0.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    demo(save=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa28d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
