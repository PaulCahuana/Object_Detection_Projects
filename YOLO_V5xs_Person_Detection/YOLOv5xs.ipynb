{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6MPjfT5NrKQ"
   },
   "source": [
    "<a align=\"left\" href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/125273437-35b3fc00-e30d-11eb-9079-46f313325424.png\"></a>\n",
    "\n",
    "This is the **official YOLOv5 ðŸš€ notebook** authored by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
    "For more information please visit https://github.com/ultralytics/yolov5 and https://ultralytics.com. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone repo, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "0cabe440-e06c-48b9-9180-4b4ea1790ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.1 (NVIDIA GeForce GTX 1660 SUPER)\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "#%pip install -qr requirements.txt  # install dependencies\n",
    "\n",
    "import torch\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "clear_output()\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Inference\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/114307955-5c7e4e80-9ae2-11eb-9f50-a90e39bee53f.png\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "c9a308f7-2216-4805-8003-eca8dd0dc30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp6/weights/last_v5s.pt'], source=videos/st1.jpeg, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, tfl_int8=False\n",
      "YOLOv5 ðŸš€ v5.0-375-gd1182c4 torch 1.8.1 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5941.5MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n",
      "image 1/1 /media/paul/disco5/ANACONDA/LYTICA/YOLOV5_XS_STEREN/yolov5/videos/st1.jpeg: 384x640 11 persons, Done. (0.008s)\n",
      "Results saved to \u001b[1mruns/detect/exp8\u001b[0m\n",
      "Done. (0.038s)\n",
      "00:00:03.92\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time() #tomamos medida del tiempo\n",
    "\n",
    "#!python detect.py --weights runs/train/exp6/weights/last.pt --img 640 --conf 0.40 --source videos/centro1.mp4\n",
    "#!python detect.py --weights runs/train/exp6/weights/last.pt --img 640 --conf 0.40 --source videos/st1.jpeg\n",
    "#!python detect.py --weights runs/train/exp6/weights/last_v5s.pt --img 640 --conf 0.40 --source videos/centro1.mp4\n",
    "!python detect.py --weights runs/train/exp6/weights/last_v5s.pt --img 640 --conf 0.40 --source videos/st1.jpeg\n",
    "end = time.time() #paramos el tiempo\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Validate\n",
    "Validate a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyTZYGgRjnMc"
   },
   "source": [
    "## COCO val2017\n",
    "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cef5e9351ca743bcba5febac0b096a30",
      "ec326c52378f4410920c328f221e0514",
      "83000c64a11c4ae8abd6f0ef2f108cef",
      "0f7899eb719f4a9c9852426551f97be9",
      "886ac5b18b3c4c82bf15ad5055f1e17e",
      "4e67b3c3a49849c7a7ba28b7eec96e7a",
      "62c3682ff1804571a483d46664533969",
      "599dda3b608b432393760b2ca4ae7c7d"
     ]
    },
    "id": "WQPtK1QYVaD_",
    "outputId": "56b6402a-81d5-41d0-a3c8-8889db1fca6c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef5e9351ca743bcba5febac0b096a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=819257867.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download COCO val2017\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X58w8JLpMnjH",
    "outputId": "a5d41761-f1a0-41fe-d0bb-4cceebd7c4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', exist_ok=False, half=True, img_size=640, iou_thres=0.65, name='exp', project='runs/val', save_conf=False, save_hybrid=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt'])\n",
      "YOLOv5 ðŸš€ v5.0-157-gc6b51f4 torch 1.8.1+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n",
      "100% 168M/168M [00:01<00:00, 156MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 3008.87it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../coco/val2017.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:17<00:00,  2.02it/s]\n",
      "                 all       5000      36335      0.746      0.626       0.68       0.49\n",
      "Speed: 5.3/1.5/6.8 ms inference/NMS/total per 640x640 image at batch-size 32\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=4.88s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=83.47s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=12.96s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n",
      "Results saved to runs/val/exp\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5x on COCO val2017\n",
    "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc_KbFk0juX2"
   },
   "source": [
    "## COCO test-dev2017\n",
    "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0AJnSeCIHyJ"
   },
   "outputs": [],
   "source": [
    "# Download COCO test-dev2017\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip # unzip labels\n",
    "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n",
    "%mv ./test2017 ../coco/images  # move to /coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29GJXAP_lPrt"
   },
   "outputs": [],
   "source": [
    "# Run YOLOv5s on COCO test-dev2017 using --task test\n",
    "!python val.py --weights yolov5s.pt --data coco.yaml --task test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOiNLtMP5aG"
   },
   "source": [
    "# 3. Train\n",
    "\n",
    "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "217ca488c82a4b7a80318b70887a556e",
      "4e63af16f1084ca98a6fa5a282f2a81e",
      "49f4b3c7f6ff42b4b9132a8550e12186",
      "8ec9e1a4883245daaf029458ee09721f",
      "9d3e775ee11e4cf4b587b64fbc3cc6f7",
      "70f68a9a51ac46e6ab7e51fb4fc6bda3",
      "fdb8ab377c114bc3b862ba76eb93cef7",
      "cd267c153c244621a1f50706d2ddc897"
     ]
    },
    "id": "Knxi2ncxWffW",
    "outputId": "9e4788c2-e1d4-4a13-c3d2-984f5df7ffab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217ca488c82a4b7a80318b70887a556e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22091032.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download COCO128\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pOkGLv1dMqh"
   },
   "source": [
    "Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
    "\n",
    "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOy5KI2ncnWd"
   },
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fLAV42oNb7M"
   },
   "outputs": [],
   "source": [
    "# Weights & Biases  (optional)\n",
    "%pip install -q wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "c4dfc591-b6f9-4a60-9149-ee7eff970c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (offline), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 ðŸš€ v5.0-375-gd1182c4 torch 1.8.1 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5941.5MB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "2021-08-20 16:15:21.084426: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.1/lib64\n",
      "2021-08-20 16:15:21.084453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs\n",
      "\n",
      "Transferred 356/362 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../train/labels/train.cache' images and labels... 1126 found, 0\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram): 100%|â–ˆâ–ˆ| 1126/1126 [00:00<00:00, 2124.27it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../train/labels/val.cache' images and labels... 321 found, 0 miss\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 321/321 [00:00<00:00, 409.31it/s]\u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.21, Best Possible Recall (BPR) = 1.0000\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/exp6\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/49     3.35G   0.09858    0.0569         0        40       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.649       0.53      0.527      0.198\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/49     3.55G   0.06604    0.0536         0        31       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.828      0.638      0.772      0.344\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/49     3.55G   0.05873   0.03985         0        57       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.767      0.684      0.792      0.372\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/49     3.55G   0.05526   0.03614         0        51       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.804      0.773      0.849       0.47\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/49     3.55G   0.05009   0.03503         0        55       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.868      0.821      0.901      0.525\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/49     3.55G   0.04691   0.03358         0        32       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.896      0.818      0.905      0.535\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/49     3.55G   0.04363   0.03298         0        44       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.881      0.845      0.914      0.565\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/49     3.55G   0.04794   0.03386         0        56       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.728      0.843      0.852      0.474\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/49     3.55G    0.0458    0.0323         0        41       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.882      0.843      0.915      0.552\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/49     3.55G   0.04273   0.03221         0        40       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.86      0.845      0.908      0.509\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/49     3.55G   0.04014   0.03214         0        51       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.898      0.831      0.922      0.572\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/49     3.55G    0.0393   0.03165         0        44       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.891      0.848      0.929      0.573\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/49     3.55G   0.04043   0.03138         0        37       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.873      0.853      0.929      0.582\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/49     3.55G   0.03849   0.03122         0        50       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.854      0.874      0.928      0.603\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/49     3.55G   0.03752   0.03014         0        43       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.895      0.833      0.923      0.549\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/49     3.55G   0.03653   0.03008         0        58       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.899      0.836      0.927      0.595\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/49     3.55G   0.03451   0.03101         0        38       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.892       0.84      0.933      0.606\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/49     3.55G   0.03186   0.02959         0        47       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.89      0.852      0.929       0.61\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/49     3.55G   0.03107   0.02954         0        19       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.883      0.855       0.93      0.615\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/49     3.55G   0.03061   0.02851         0        69       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.88      0.855       0.93      0.561\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/49     3.55G   0.03078   0.02896         0        61       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.89      0.847      0.931      0.633\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/49     3.55G   0.03035   0.02874         0        41       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.899      0.844      0.935      0.638\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/49     3.55G   0.02946   0.02726         0        56       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.878      0.858      0.933      0.631\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/49     3.55G   0.02974   0.02795         0        42       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.86      0.873      0.931       0.64\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/49     3.55G   0.02857   0.02721         0        54       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.878      0.858      0.932      0.631\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/49     3.55G   0.02818   0.02776         0        58       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.886      0.847      0.935      0.641\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/49     3.55G   0.02751   0.02731         0        22       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.884      0.856      0.935      0.651\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/49     3.55G   0.02707   0.02721         0        48       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.899      0.839      0.933      0.648\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/49     3.55G   0.02657   0.02707         0        38       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.865      0.866      0.933      0.651\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/49     3.55G   0.02607   0.02645         0        37       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.872       0.86      0.935      0.655\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     30/49     3.55G   0.02649   0.02675         0        46       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.869      0.875      0.934      0.656\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     31/49     3.55G    0.0257   0.02507         0        48       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.887      0.851      0.932      0.644\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     32/49     3.55G    0.0252   0.02611         0        54       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.887      0.859      0.935      0.658\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     33/49     3.55G   0.02515   0.02577         0        40       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.886      0.865      0.938      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     34/49     3.55G   0.02508    0.0252         0        53       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.85      0.887      0.938      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     35/49     3.55G   0.02508   0.02541         0        26       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.893       0.86      0.939      0.663\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     36/49     3.55G   0.02456   0.02468         0        43       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.893      0.854      0.937      0.665\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     37/49     3.55G   0.02445   0.02491         0        53       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.89      0.859      0.937      0.656\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     38/49     3.55G   0.02427   0.02421         0        35       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.89      0.856      0.938      0.661\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     39/49     3.55G   0.02394   0.02472         0        55       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.866      0.871      0.934      0.658\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     40/49     3.55G   0.02392   0.02441         0        57       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.885      0.865      0.934      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     41/49     3.55G   0.02403   0.02546         0        61       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.861      0.874      0.932      0.655\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     42/49     3.55G   0.02346    0.0243         0        33       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.909      0.831      0.935      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     43/49     3.55G   0.02354   0.02438         0        57       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.865       0.87      0.934      0.661\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     44/49     3.55G    0.0233   0.02445         0        56       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.882      0.859      0.936      0.671\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     45/49     3.55G    0.0236   0.02413         0        41       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.869      0.872      0.935      0.666\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     46/49     3.55G    0.0234   0.02358         0        26       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.854      0.889      0.931      0.663\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     47/49     3.55G   0.02305   0.02401         0        31       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158      0.871      0.877      0.936      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     48/49     3.55G   0.02344   0.02381         0        41       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.88      0.869      0.934      0.666\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     49/49     3.55G   0.02304   0.02349         0        51       640: 100%|â–ˆ| \n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        321       1158       0.86      0.886      0.935       0.67\n",
      "\n",
      "50 epochs completed in 2.380 hours.\n",
      "Optimizer stripped from runs/train/exp6/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from runs/train/exp6/weights/best.pt, 14.4MB\n",
      "Results saved to \u001b[1mruns/train/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 16 --epochs 50 --data custom_data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15glLzbQx5u0"
   },
   "source": [
    "# 4. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "## Weights & Biases Logging ðŸŒŸ NEW\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
    "\n",
    "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/125274843-a27bc600-e30e-11eb-9a44-62af0b7a50a2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WPvRbS5Swl6"
   },
   "source": [
    "## Local Logging\n",
    "\n",
    "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and val jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riPdhraOTCO0"
   },
   "outputs": [],
   "source": [
    "Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n",
    "Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # val batch 0 labels\n",
    "Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # val batch 0 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYG4WFEnTVrI"
   },
   "source": [
    "> <img src=\"https://user-images.githubusercontent.com/26833433/124931219-48bf8700-e002-11eb-84f0-e05d95b118dd.jpg\" width=\"700\">  \n",
    "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/124931217-4826f080-e002-11eb-87b9-ae0925a8c94b.jpg\" width=\"700\">  \n",
    "`test_batch0_labels.jpg` shows val batch 0 labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/124931209-46f5c380-e002-11eb-9bd5-7a3de2be9851.jpg\" width=\"700\">  \n",
    "`test_batch0_pred.jpg` shows val batch 0 _predictions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KN5ghjE6ZWh"
   },
   "source": [
    "Training results are automatically logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and `runs/train/exp/results.txt`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.txt` file manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDznIqPF7nk3"
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_results \n",
    "plot_results(save_dir='runs/train/exp')  # plot all results*.txt files in 'runs/train/exp'\n",
    "Image(filename='runs/train/exp/results.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfrEegCSW3fK"
   },
   "source": [
    "<p align=\"left\"><img width=\"800\" alt=\"COCO128 Training Results\" src=\"https://user-images.githubusercontent.com/26833433/125273596-6300aa00-e30d-11eb-8dc4-70a960c53013.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zelyeqbyt3GD"
   },
   "source": [
    "# Environments\n",
    "\n",
    "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
    "\n",
    "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
    "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
    "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
    "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qu7Iesl0p54"
   },
   "source": [
    "# Status\n",
    "\n",
    "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
    "\n",
    "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEijrePND_2I"
   },
   "source": [
    "# Appendix\n",
    "\n",
    "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcKoSIK2WSzj"
   },
   "outputs": [],
   "source": [
    "# Reproduce\n",
    "for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.25 --iou 0.45  # speed\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "outputs": [],
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Images\n",
    "dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGH0ZjkGjejy"
   },
   "outputs": [],
   "source": [
    "# Unit tests\n",
    "%%shell\n",
    "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
    "\n",
    "rm -rf runs  # remove runs/\n",
    "for m in yolov5s; do  # models\n",
    "  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n",
    "  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n",
    "  for d in 0 cpu; do  # devices\n",
    "    python detect.py --weights $m.pt --device $d  # detect official\n",
    "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
    "    python val.py --weights $m.pt --device $d # val official\n",
    "    python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom\n",
    "  done\n",
    "  python hubconf.py  # hub\n",
    "  python models/yolo.py --cfg $m.yaml  # inspect\n",
    "  python export.py --weights $m.pt --img 640 --batch 1  # export\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gogI-kwi3Tye"
   },
   "outputs": [],
   "source": [
    "# Profile\n",
    "from utils.torch_utils import profile \n",
    "\n",
    "m1 = lambda x: x * torch.sigmoid(x)\n",
    "m2 = torch.nn.SiLU()\n",
    "profile(x=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVRSOhEvUdb5"
   },
   "outputs": [],
   "source": [
    "# Evolve\n",
    "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
    "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSgFCAcMbk1R"
   },
   "outputs": [],
   "source": [
    "# VOC\n",
    "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
    "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f7899eb719f4a9c9852426551f97be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_599dda3b608b432393760b2ca4ae7c7d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_62c3682ff1804571a483d46664533969",
      "value": " 781M/781M [00:12&lt;00:00, 67.1MB/s]"
     }
    },
    "217ca488c82a4b7a80318b70887a556e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49f4b3c7f6ff42b4b9132a8550e12186",
       "IPY_MODEL_8ec9e1a4883245daaf029458ee09721f"
      ],
      "layout": "IPY_MODEL_4e63af16f1084ca98a6fa5a282f2a81e"
     }
    },
    "49f4b3c7f6ff42b4b9132a8550e12186": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70f68a9a51ac46e6ab7e51fb4fc6bda3",
      "max": 22091032,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d3e775ee11e4cf4b587b64fbc3cc6f7",
      "value": 22091032
     }
    },
    "4e63af16f1084ca98a6fa5a282f2a81e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e67b3c3a49849c7a7ba28b7eec96e7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "599dda3b608b432393760b2ca4ae7c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62c3682ff1804571a483d46664533969": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70f68a9a51ac46e6ab7e51fb4fc6bda3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83000c64a11c4ae8abd6f0ef2f108cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e67b3c3a49849c7a7ba28b7eec96e7a",
      "max": 819257867,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_886ac5b18b3c4c82bf15ad5055f1e17e",
      "value": 819257867
     }
    },
    "886ac5b18b3c4c82bf15ad5055f1e17e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8ec9e1a4883245daaf029458ee09721f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd267c153c244621a1f50706d2ddc897",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fdb8ab377c114bc3b862ba76eb93cef7",
      "value": " 21.1M/21.1M [00:36&lt;00:00, 605kB/s]"
     }
    },
    "9d3e775ee11e4cf4b587b64fbc3cc6f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cd267c153c244621a1f50706d2ddc897": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cef5e9351ca743bcba5febac0b096a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83000c64a11c4ae8abd6f0ef2f108cef",
       "IPY_MODEL_0f7899eb719f4a9c9852426551f97be9"
      ],
      "layout": "IPY_MODEL_ec326c52378f4410920c328f221e0514"
     }
    },
    "ec326c52378f4410920c328f221e0514": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdb8ab377c114bc3b862ba76eb93cef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
